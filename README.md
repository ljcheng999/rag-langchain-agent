# LangGraph FastAPI Integration - Intelligent RAG Agent

A sophisticated AI agent system that integrates FastAPI with LangChain/LangGraph to provide intelligent routing between Retrieval-Augmented Generation (RAG), web search, and direct answering capabilities. This project demonstrates advanced conversational AI with context-aware decision making.

---
# ğŸ—ï¸ Architecture Overview
![architecture](./assets/advanced-rag-agent-graph.png)

```
User Query â†’ Router â†’ [RAG | Web Search | Direct Answer] â†’ Response
```
---

## ğŸš€ Features
- **ğŸ§  Intelligent Routing**: The system will do decision-making between RAG, web search, and direct answering automatically
- **ğŸ“š RAG Integration**: Advanced document search using ChromaDB vector store with contextual query reformulation
- **ğŸŒ Web Search**: Real-time information retrieval using Tavily API
- **ğŸ’¬ Session Management**: Persistent conversation history across requests
- **ğŸ“„ Document Management**: Upload, index, list, and delete documents
- **ğŸ” Contextual Query Processing**: Smart query reformulation for follow-up questions
- **ğŸ“Š Comprehensive Logging**: Detailed logging for debugging and analysis and save into a log file
- **ğŸ—ï¸ Modular Architecture**: Clean separation of concerns with dedicated modules

---

### Core Components

1. **Router Node**: Analyzes queries and routes to appropriate handler
2. **RAG Node**: Searches document knowledge base with intelligent judging
3. **Web Search Node**: Performs real-time web searches
4. **Answer Node**: Synthesizes information into coherent responses

---
## ğŸ“‹ Prerequisites

- Python 3.8+
- OpenAI API Key
- Tavily API Key (for web search functionality)
- Git

---

### ğŸ› ï¸ Installation & Setup (local machine)

#### 1. Clone the Repository

```bash
git clone <repository-url>
cd rad-langchain-agent
```

#### 2. Install Dependencies

Using `uv` (recommended):
```bash
pip/pip3 install uv
uv pip install -r server/requirements.txt
```

Or using pip(pip3):
```bash
pip/pip3 install -r api/requirements.txt
```

#### 3. Environment Configuration
Rename .env.example file to .env, and fill out the empty strings

#### 4. Initialize the Application

The database and vector store will be created automatically on first run.

## ğŸš€ Running the Application

### Development Server

```bash
cd server
uvicorn main:app --reload --host 0.0.0.0 --port 8080
```

### Production Server

```bash
cd server
uvicorn main:app --host 0.0.0.0 --port 8080
```

### Production/Development Server (Docker)
```bash
docker pull jcheng919/rag-langchain-fastapi:v1.0.1
docker run --rm --name rag-langchain-agent \
-e OPENAI_API_KEY="" \
-e LANGCHAIN_TRACING_V2="true" \
-e LANGCHAIN_ENDPOINT="https://api.smith.langchain.com" \
-e LANGCHAIN_API_KEY="" \
-e LANGCHAIN_PROJECT="" \
-e TAVILY_API_KEY="" \
-p 8080:8080 \
rag-langchain-fastapi:v1.0.1
```

The API will be available at `http://localhost:8080`

---
## ğŸ“š API Documentation

### Interactive API Docs

Once running, visit:
- **Swagger UI**: `http://localhost:8080/docs`
- **ReDoc**: `http://localhost:8080/redoc`

### 1. Chat Endpoint

**POST** `/chat`

The main conversational endpoint using the intelligent LangGraph agent.

**Request Body:**
```json
{
  "question": "What is the GreenGrow company?",
  "session_id": "user-123",
  "model": "gpt-4.1-mini"
}
```

**Response:**
```json
{
  "answer": "The GreenGrow company...",
  "session_id": "user-123",
  "model": "gpt-4.1-mini"
}
```

**Features:**
- Automatic query contextualization for follow-up questions
- Intelligent routing based on query type
- Session-based conversation history
- Multiple model support

### 2. Document Management

#### Upload Document
**POST** `/upload-doc`

Upload and index documents for RAG functionality.

```bash
curl -X POST "http://localhost:8080/upload-doc" \
  -F "file=@your_document.pdf"
```

**Supported Formats:** PDF, DOCX, HTML

#### List Documents
**GET** `/list-docs`

Retrieve all uploaded documents.

```bash
curl -X GET "http://localhost:8080/list-docs"
```

#### Delete Document
**POST** `/delete-doc`

Remove documents from the system.

```bash
curl -X POST "http://localhost:8080/delete-doc" \
  -H "Content-Type: application/json" \
  -d '{"file_id": 1}'
```



## ğŸ“ Project Structure

```
LangGraph FastAPI Integration/
â”œâ”€â”€ server/                       # Main application directory
â”‚   â”œâ”€â”€ main.py                   # FastAPI application with endpoints
â”‚   â”œâ”€â”€ langgraph_agent.py        # LangGraph agent definition
â”‚   â”œâ”€â”€ nodes.py                  # Agent node implementations
â”‚   â”œâ”€â”€ tools.py                  # RAG and web search tools
â”‚   â”œâ”€â”€ pydantic_models.py        # API request/response models
â”‚   â”œâ”€â”€ db_utils.py               # Database operations
â”‚   â”œâ”€â”€ chroma_utils.py           # Chroma vector store operations
â”‚   â”œâ”€â”€ langchain_utils.py        # LangChain utilities
â”‚   â”œâ”€â”€ shared.py                 # Shared configurations and types
â”‚   â”œâ”€â”€ utils.py                  # Utility functions
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â”œâ”€â”€ app.log                   # Application logs
â”œâ”€â”€ docs/                         # Sample documents for testing
â”‚   â”œâ”€â”€ Company_GreenFieldsBioTech.docx
â”‚   â”œâ”€â”€ Company_QuantumNextSystems.docx
â”‚   â”œâ”€â”€ Company_TechWaveInnovations.docx
â”‚   â”œâ”€â”€ GreenGrowInnovations_Company History.docx
â”‚   â””â”€â”€ GreenGrow'sEcoHarvestSystem_ARevolutioninFarming.pdf
â””â”€â”€ README.md                     # This current file
```

## ğŸ”„ Agent Workflow

### 1. Query Processing
- User submits question
- System contextualizes query using chat history
- Router analyzes and determines optimal path

### 2. Routing Decision
- **`end`**: Greetings/small talk â†’ Direct response
- **`rag`**: Document queries â†’ Knowledge base search
- **`answer`**: General questions â†’ Direct AI response

### 3. Information Retrieval
- **RAG Node**: Searches uploaded documents
- **Web Search Node**: Retrieves current information
- **Intelligent Judging**: Determines if RAG results are sufficient

### 4. Response Generation
- Synthesizes information from multiple sources
- Generates coherent, contextual responses
- Maintains conversation history

## ğŸ’¡ Usage Examples

### Example 1: Document Query
```json
{
  "question": "What is the GreenGrow System?",
  "session_id": "user-123"
}
```
**Flow**: `router` â†’ `rag` â†’ `answer`

### Example 2: Follow-up Question
```json
{
  "question": "When was it introduced?",
  "session_id": "user-123"
}
```
**Flow**: Contextualization â†’ `router` â†’ `rag` â†’ `answer`

### Example 3: Current Events
```json
{
  "question": "What's the latest news about AI developments?",
  "session_id": "user-123"
}
```
**Flow**: `router` â†’ `rag` â†’ `web` â†’ `answer`

### Example 4: Simple Question
```json
{
  "question": "What is the capital of France?",
  "session_id": "user-123"
}
```
**Flow**: `router` â†’ `answer`

## ğŸ—„ï¸ Database Schema

### Chat History Table
Tracks all user interactions for conversation continuity.

| Column | Type | Description |
|--------|------|-------------|
| `id` | INTEGER | Primary key, auto-increment |
| `session_id` | TEXT | Conversation session identifier |
| `user_query` | TEXT | User's question |
| `gpt_response` | TEXT | AI model response |
| `model` | TEXT | AI model used |
| `created_at` | TIMESTAMP | Timestamp of interaction |

### Document Store Table
Manages uploaded documents for RAG functionality.

| Column | Type | Description |
|--------|------|-------------|
| `id` | INTEGER | Primary key, auto-increment |
| `filename` | TEXT | Document filename |
| `upload_timestamp` | TIMESTAMP | Upload timestamp |

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `OPENAI_API_KEY` | OpenAI API key for LLM access | Yes |
| `TAVILY_API_KEY` | Tavily API key for web search | No* |

*Required for web search functionality

### Model Options

- `gpt-4.1`: Full GPT-4 model for complex queries
- `gpt-4.1-mini`: Optimized model for faster responses

## ğŸ› Troubleshooting

### Common Issues

1. **Missing API Keys**
   ```
   Error: OpenAI API key not found
   Solution: Set OPENAI_API_KEY in .env file
   ```

2. **Tavily Search Errors**
   ```
   Error: Tavily API key required for web search
   Solution: Set TAVILY_API_KEY or disable web search
   ```

3. **Chroma Vector Store Issues**
   ```
   Error: Vector store not found
   Solution: Vector store is auto-created in ./chroma_db
   ```

4. **Database Errors**
   ```
   Error: Database connection failed
   Solution: SQLite database is auto-created
   ```

### Logs

Check `server/app.log` for detailed application logs and debugging information.

## ğŸ”’ Security Considerations

- Store API keys in environment variables
- Validate file uploads (type and size)
- Implement rate limiting for production
- Use HTTPS in production environments
- Regular security updates for dependencies

## ğŸš€ Performance Optimization

- Use `gpt-4.1-mini` for faster responses
- Implement caching for frequent queries
- Optimize vector store queries
- Monitor memory usage with large document sets

**Happy coding! ğŸš€** 